{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q dlai-grader==1.20.0 tensorflow-text==2.17.0 tensorflow==2.17.0 numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnaUzIjgBFnz",
        "outputId": "6bd48b95-4047-47fe-d71d-76310419109d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FI1nBugjcSEZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Setting this env variable prevents TF warnings from showing up\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "\n",
        "from utils import (train_data, val_data, portuguese_vectorizer,\n",
        "                         english_vectorizer, masked_loss, masked_acc, tokens_to_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "wEl3wNfmlNEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# StringLookup used to get word from ids and vice versa\n",
        "id_to_word = tf.keras.layers.StringLookup(\n",
        "    vocabulary = portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token = \"\",\n",
        "    oov_token = \"[UNK]\",\n",
        "    invert = True)"
      ],
      "metadata": {
        "id": "xJSY0xDPC9If"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (trans_sample_cont, trans_sample_pre), trans_sample_post in train_data.take(1):\n",
        "\n",
        "    print(f\"Tokenized english sentence:\\n{trans_sample_cont[0, :].numpy()}\\n\\n\")\n",
        "\n",
        "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{trans_sample_pre[0, :].numpy()}\\n\\n\")\n",
        "\n",
        "    print(f\"Tokenized portuguese sentence:\\n{trans_sample_post[0, :].numpy()}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZcdlMTdlXb3",
        "outputId": "695dbdcb-2c59-4b55-f8ab-373cc9420081"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized english sentence:\n",
            "[  2  13 300  59 130   8   7   9 952   4   3   0   0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence (shifted to the right):\n",
            "[   2  237  243   47   57  299   35 1024    4    0    0    0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence:\n",
            "[ 237  243   47   57  299   35 1024    4    3    0    0    0]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NMT model with attention"
      ],
      "metadata": {
        "id": "HVh4ZMpJloO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, units):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, units, mask_zero = True)\n",
        "        self.lstm = tf.keras.layers.LSTM(units = units, return_sequences = True)\n",
        "        self.rnn = tf.keras.layers.Bidirectional(layer = self.lstm, merge_mode = 'sum')\n",
        "\n",
        "    def call(self, context):\n",
        "\n",
        "        x = self.embedding(context)\n",
        "        x = self.rnn(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "c9Uh9uL3fvsX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "units = 256\n",
        "vocab_size = english_vectorizer.vocabulary_size()"
      ],
      "metadata": {
        "id": "J_qfu6xJij-s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size, units)\n",
        "encoder_out = encoder(trans_sample_cont)\n",
        "\n",
        "print(f'Encoder output has shape: {encoder_out.shape}') # (batch_size , seq_len , hidden_units)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH-aGda8yxH-",
        "outputId": "8445b6cb-a262-4131-8e71-7c1fcb0ce504"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output has shape: (64, 13, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A couple of things to notice:**\n",
        "\n",
        "\n",
        "*   You need a way to pass both the output of the attention alongside the shifted-to-the-right translation (since this cross attention happens in the decoder side). For this you will use an Add layer so that the original dimension is preserved, which would not happen if you use something like a Concatenate layer.\n",
        "*   Layer normalization is also performed for better stability of the network by using a LayerNormalization layer."
      ],
      "metadata": {
        "id": "4lJEVtaHO10-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads = 1,\n",
        "            key_dim = units)\n",
        "\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, context, target):\n",
        "\n",
        "        attn_output = self.mha(value = context, query = target)\n",
        "\n",
        "        x = self.add([target, attn_output])\n",
        "\n",
        "        x =  self.layernorm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JVZjZNxB2EFk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = CrossAttention(units)\n",
        "trans_sample_pre_embed = tf.keras.layers.Embedding(vocab_size, output_dim=units, mask_zero=True)(trans_sample_pre)\n",
        "\n",
        "attention_result = attention_layer(encoder_out, trans_sample_pre_embed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsSxXJQXDG-Z",
        "outputId": "3b9cbb91-54f1-46c6-8684-78f3d3b1c55a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Tensor of contexts has shape: {encoder_out.shape}')\n",
        "print(f'Tensor of translations has shape: {trans_sample_pre_embed.shape}')\n",
        "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBxHXfo_Q8Fk",
        "outputId": "0b98b95b-392b-48f7-e9b3-7f420b0ab7ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of contexts has shape: (64, 13, 256)\n",
            "Tensor of translations has shape: (64, 12, 256)\n",
            "Tensor of attention scores has shape: (64, 12, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, units):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            vocab_size, units, mask_zero = True\n",
        "        )\n",
        "\n",
        "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units,\n",
        "            return_sequences = True,\n",
        "            return_state = True\n",
        "        )\n",
        "\n",
        "        self.attention = CrossAttention(units)\n",
        "\n",
        "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units,\n",
        "            return_sequences = True\n",
        "        )\n",
        "\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            vocab_size,\n",
        "            activation = tf.nn.log_softmax\n",
        "        )\n",
        "\n",
        "    def call(self, context, target, state=None, return_state=False):\n",
        "\n",
        "        x = self.embedding(target)\n",
        "\n",
        "        x, hidden_states, cell_states = self.pre_attention_rnn(x, initial_state = state)\n",
        "\n",
        "        x = self.attention(context, x)\n",
        "\n",
        "        x = self.post_attention_rnn(x)\n",
        "\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        if return_state:\n",
        "            return  logits, [hidden_states, cell_states]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "sfuwSVmMKOCt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_size, units)\n",
        "logits = decoder(context = encoder_out, target = trans_sample_pre)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiICMfs3buKc",
        "outputId": "5060c92c-63b2-4f43-a430-f17e919e9573"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_1' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'decoder' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Tensor of contexts has shape: {encoder_out.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {trans_sample_pre.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmOkmCrCbqGC",
        "outputId": "03ea210d-bb35-4290-a4b2-33f390058884"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of contexts has shape: (64, 13, 256)\n",
            "Tensor of right-shifted translations has shape: (64, 12)\n",
            "Tensor of logits has shape: (64, 12, 12000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, units):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(vocab_size, units)\n",
        "\n",
        "        self.decoder = Decoder(vocab_size, units)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        context, target = inputs\n",
        "\n",
        "        encoded_context = self.encoder(context)\n",
        "\n",
        "        logits = self.decoder(encoded_context, target)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JJe53wPQeiNb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(vocab_size, units)\n",
        "\n",
        "logits = translator((trans_sample_cont, trans_sample_pre))\n",
        "\n",
        "print(f'Tensor of sentences to translate has shape: {trans_sample_cont.shape}')\n",
        "print(f'Tensor of right-shifted translations has shape: {trans_sample_pre.shape}')\n",
        "print(f'Tensor of logits has shape: {logits.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuxH_Y81GzFo",
        "outputId": "46a0ddd5-9437-4fba-9bfe-88f803b0ad21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'encoder_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'cross_attention_2' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor of sentences to translate has shape: (64, 13)\n",
            "Tensor of right-shifted translations has shape: (64, 12)\n",
            "Tensor of logits has shape: (64, 12, 12000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'decoder_1' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "l-S-POkcmBzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_and_train(model, train_data, validation_data, optimizer, loss_function, metrics: list,\n",
        "                      epochs = 20, steps_per_epoch = 500, validation_steps = 50):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer = optimizer,\n",
        "        loss = loss_function,\n",
        "        metrics = metrics\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data.repeat(),\n",
        "        epochs = epochs,\n",
        "        steps_per_epoch = steps_per_epoch,\n",
        "        validation_data = validation_data,\n",
        "        validation_steps = validation_steps,\n",
        "        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3)]\n",
        "    )\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "Y1SxXoN8JT1A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_translator, history = compile_and_train(\n",
        "    model = translator,\n",
        "    train_data = train_data,\n",
        "    validation_data = val_data,\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    loss_function = masked_loss,\n",
        "    metrics = [masked_acc, masked_loss],\n",
        "    epochs = 10,\n",
        "    steps_per_epoch = 500,\n",
        "    validation_steps = 50\n",
        "    )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D2u0ocEY0d21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b06037-f3e5-4015-e92e-c48cfa246691"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 68ms/step - loss: 5.7520 - masked_acc: 0.1774 - masked_loss: 5.7520 - val_loss: 4.3547 - val_masked_acc: 0.3285 - val_masked_loss: 4.3547\n",
            "Epoch 2/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 66ms/step - loss: 4.0982 - masked_acc: 0.3659 - masked_loss: 4.0982 - val_loss: 3.2017 - val_masked_acc: 0.4786 - val_masked_loss: 3.2017\n",
            "Epoch 3/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 66ms/step - loss: 3.0342 - masked_acc: 0.5026 - masked_loss: 3.0342 - val_loss: 2.4403 - val_masked_acc: 0.5820 - val_masked_loss: 2.4403\n",
            "Epoch 4/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - loss: 2.3615 - masked_acc: 0.5972 - masked_loss: 2.3615 - val_loss: 2.0118 - val_masked_acc: 0.6369 - val_masked_loss: 2.0118\n",
            "Epoch 5/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - loss: 1.9935 - masked_acc: 0.6489 - masked_loss: 1.9935 - val_loss: 1.7712 - val_masked_acc: 0.6742 - val_masked_loss: 1.7712\n",
            "Epoch 6/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 67ms/step - loss: 1.6974 - masked_acc: 0.6873 - masked_loss: 1.6974 - val_loss: 1.5867 - val_masked_acc: 0.7030 - val_masked_loss: 1.5867\n",
            "Epoch 7/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - loss: 1.5624 - masked_acc: 0.7065 - masked_loss: 1.5624 - val_loss: 1.4682 - val_masked_acc: 0.7209 - val_masked_loss: 1.4682\n",
            "Epoch 8/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - loss: 1.4350 - masked_acc: 0.7240 - masked_loss: 1.4350 - val_loss: 1.4032 - val_masked_acc: 0.7265 - val_masked_loss: 1.4032\n",
            "Epoch 9/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - loss: 1.3665 - masked_acc: 0.7331 - masked_loss: 1.3665 - val_loss: 1.3142 - val_masked_acc: 0.7389 - val_masked_loss: 1.3142\n",
            "Epoch 10/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - loss: 1.2618 - masked_acc: 0.7460 - masked_loss: 1.2618 - val_loss: 1.2486 - val_masked_acc: 0.7497 - val_masked_loss: 1.2486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"model.weights.h5\"\n",
        "trained_translator.save_weights(save_dir)"
      ],
      "metadata": {
        "id": "dJkcGsx21_zx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator.load_weights(save_dir)"
      ],
      "metadata": {
        "id": "-s7unKT5EuDd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
        "\n",
        "    \"Generates the next token in the sequence\"\n",
        "\n",
        "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
        "    logits = logits[:,-1,:]\n",
        "\n",
        "    # If temp is 0 then next_token is the argmax of logits\n",
        "    if temperature == 0.0:\n",
        "        next_token = tf.argmax(logits, axis = -1)\n",
        "\n",
        "    # If temp is not 0 so temperature will used\n",
        "    else:\n",
        "        logits = logits / temperature\n",
        "        next_token = tf.random.categorical(logits, num_samples = 1)\n",
        "\n",
        "    logits = tf.squeeze(logits)\n",
        "    next_token = tf.squeeze(next_token)\n",
        "\n",
        "    # get the logit of the selected next_token\n",
        "    logit = logits[next_token].numpy()\n",
        "\n",
        "    next_token = tf.reshape(next_token, shape = (1,1))\n",
        "\n",
        "    # If next_token is End-of-Sentence token you are done\n",
        "    if next_token == 3: # EOS id\n",
        "        done = True\n",
        "\n",
        "    return next_token, logit, state, done"
      ],
      "metadata": {
        "id": "yIoeoH3CFal6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = english_vectorizer(['i love languages']).to_tensor()\n",
        "next_token = tf.convert_to_tensor([[2]]) # SOS id\n",
        "\n",
        "context = encoder(context)\n",
        "state = [tf.random.uniform((1, units)), tf.random.uniform((1, units))]\n",
        "\n",
        "next_token, logit, state, done = generate_next_token(decoder, context, next_token, False, state)"
      ],
      "metadata": {
        "id": "hutAzARSbWWf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKQzMAgmbrWl",
        "outputId": "14a16aeb-cf0f-4b44-f15b-5465b09cc98d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token: [[9529]]\n",
            "Logit: -9.2947\n",
            "Done? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the model for inference"
      ],
      "metadata": {
        "id": "yc3Z43HRoF5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, text, max_length = 50, temperature=0.0):\n",
        "\n",
        "    \"Translate a given sentence from English to Portuguese\"\n",
        "\n",
        "    tokens, logits = [], []\n",
        "\n",
        "    context = english_vectorizer(tf.constant([text])).to_tensor()\n",
        "    context = model.encoder(context)\n",
        "\n",
        "    next_token = tf.fill(dims = (1, 1), value = 2) # SOS id\n",
        "\n",
        "    done = False\n",
        "\n",
        "    state = [tf.zeros(shape=(1,units)) , tf.zeros(shape = (1,units))]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "\n",
        "        next_token, logit, state, done = generate_next_token(\n",
        "            decoder = model.decoder,\n",
        "            context = context,\n",
        "            next_token = next_token,\n",
        "            done = done,\n",
        "            state = state,\n",
        "            temperature = temperature\n",
        "        )\n",
        "\n",
        "        tokens.append(tf.squeeze(next_token).numpy())\n",
        "        logits.append(logit)\n",
        "\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "    # Convert the translated tokens into text\n",
        "    translation = tokens_to_text(tokens, id_to_word).numpy().decode()\n",
        "\n",
        "    return translation, logits[-1], tokens"
      ],
      "metadata": {
        "id": "79MMCnHGo26t"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = .5\n",
        "original_sentence = \"I love languages\"\n",
        "\n",
        "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
        "\n",
        "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
      ],
      "metadata": {
        "id": "sbDC2KiKSIGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8aad69-9227-4000-bfb1-51be0baabf43"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.5\n",
            "\n",
            "Original sentence: I love languages\n",
            "Translation: eu adoro linguas as linguas . [EOS]\n",
            "Translation tokens:[9, 564, 1032, 38, 1032, 4, 3]\n",
            "Logit: -0.015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Minimum Bayes-Risk Decoding"
      ],
      "metadata": {
        "id": "uNKZ2zD9ovgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(model, text, n_samples = 4, temperature = 0.6):\n",
        "\n",
        "    \"get any desired number of candidate translations alongside the log-probability for each one\"\n",
        "\n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "\n",
        "        _, logp, sample = translate(model, text, temperature = temperature)\n",
        "\n",
        "        samples.append(sample)\n",
        "        log_probs.append(logp)\n",
        "\n",
        "    return samples, log_probs"
      ],
      "metadata": {
        "id": "RkpfzCs0-9t7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples, log_probs = generate_samples(trained_translator, 'I love languages')"
      ],
      "metadata": {
        "id": "ECYMey0kImmc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
        "\n",
        "for s, l in zip(samples, log_probs):\n",
        "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqWhycwxJtiF",
        "outputId": "71901453-9f2d-41c3-d26b-5e6e301c1d31"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated tensor: [9, 564, 1032, 38, 1032, 4, 3] has logit: -0.012\n",
            "Translated tensor: [9, 564, 1032, 11, 1032, 4, 3] has logit: -0.012\n",
            "Translated tensor: [9, 564, 1032, 38, 1032, 4, 3] has logit: -0.012\n",
            "Translated tensor: [9, 564, 1032, 18, 1032, 4, 3] has logit: -0.017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "\n",
        "    \"a similarity metric, calculates the similarity between any pair of candidate and reference translations\"\n",
        "\n",
        "    candidate_set = set(candidate)\n",
        "    reference_set = set(reference)\n",
        "\n",
        "    common_tokens = candidate_set.intersection(reference_set)\n",
        "    all_tokens = candidate_set.union(reference_set)\n",
        "\n",
        "    overlap = len(common_tokens) / len(all_tokens)\n",
        "\n",
        "    return overlap"
      ],
      "metadata": {
        "id": "F10wz5VfKJPL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 3, 4]\n",
        "\n",
        "js = jaccard_similarity(l1, l2)\n",
        "\n",
        "print(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZxeKDvhMFY8",
        "outputId": "293f86fc-b234-4a92-a230-6b1c9f212ab4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rouge1_similarity(candidate, reference):\n",
        "\n",
        "    \"another similarity metric ,computes the ROUGE 1 score between candidate and reference\"\n",
        "\n",
        "    candidate_word_counts = Counter(candidate)\n",
        "    reference_word_counts = Counter(reference)\n",
        "\n",
        "    overlap = 0\n",
        "\n",
        "    for token in candidate_word_counts.keys():\n",
        "\n",
        "        token_count_candidate = candidate_word_counts[token]\n",
        "        token_count_reference = reference_word_counts[token]\n",
        "\n",
        "        overlap += min(token_count_candidate, token_count_reference)\n",
        "\n",
        "    precision = overlap / len(candidate)\n",
        "    recall = overlap / len(reference)\n",
        "\n",
        "    if precision + recall != 0: # prevent div by 0\n",
        "\n",
        "        # ROUGE-1\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        return f1_score\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "hpeJfQhpMH99"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 3, 4]\n",
        "\n",
        "r1s = rouge1_similarity(l1, l2)\n",
        "\n",
        "print(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koNwIfd6N2xO",
        "outputId": "6bd93906-4b24-4fae-beef-63035244bac0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def average_overlap(samples, similarity_fn):\n",
        "\n",
        "    \"Computes the arithmetic mean of each candidate sentence in the samples\"\n",
        "\n",
        "    iters = len(samples)\n",
        "    similarity = 0\n",
        "    scores = {}\n",
        "\n",
        "    for candidate in range(iters):\n",
        "\n",
        "        for sample in range(iters):\n",
        "\n",
        "            if candidate != sample:\n",
        "\n",
        "                similarity += similarity_fn(samples[candidate], samples[sample])\n",
        "\n",
        "        scores[candidate] = round(similarity / (iters - 1) , 3)\n",
        "        similarity = 0\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "ZBNXRQ5qXd5H"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 4]\n",
        "l3 = [1, 2, 4, 5]\n",
        "\n",
        "avg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n",
        "\n",
        "print(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkto_AfqaVsm",
        "outputId": "ecbedd49-4e42-4821-9166-9e3860b277a7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n",
            "\n",
            "{0: 0.45, 1: 0.625, 2: 0.575}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
        "\n",
        "    \"instead of arithmetic mean, compute the weighted mean of each candidate sentence in the samples\"\n",
        "\n",
        "    iters = len(samples)\n",
        "    similarity = 0\n",
        "    weights_sum = 0\n",
        "    scores = {}\n",
        "\n",
        "    for candidate in range(iters):\n",
        "\n",
        "        for sample in range(iters):\n",
        "\n",
        "            if candidate != sample:\n",
        "\n",
        "                # Convert log probability to linear scale\n",
        "                sample_p = float(np.exp(log_probs[sample]))\n",
        "\n",
        "                similarity += sample_p * similarity_fn(samples[candidate], samples[sample])\n",
        "\n",
        "                weights_sum += sample_p\n",
        "\n",
        "        # the arithmetic mean means the weight for each element is 1\n",
        "        # we are calculating the weighted mean so the weight not 1\n",
        "        # when we calculated the arithmetic mean we divided by the sum of ones\n",
        "        # in the case of weighted mean we divide by the sum of weights\n",
        "\n",
        "        scores[candidate] = round(similarity / weights_sum , 3)\n",
        "        similarity = 0\n",
        "        weights_sum = 0\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "ud79R2m4cyW8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1, 2, 3]\n",
        "l2 = [1, 2, 4]\n",
        "l3 = [1, 2, 4, 5]\n",
        "log_probs = [0.4, 0.2, 0.4]\n",
        "\n",
        "w_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n",
        "\n",
        "print(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsVzEttbnQea",
        "outputId": "aa74703f-eeab-4fa9-fc06-fc98afab8e1d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weighted average overlap using Jaccard similarity is:\n",
            "\n",
            "{0: 0.445, 1: 0.625, 2: 0.558}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mbr_decode(model, text, n_samples = 5, temperature = 0.6, similarity_fn = jaccard_similarity):\n",
        "\n",
        "    \"get translations with Minimum Bayes-Risk\"\n",
        "\n",
        "    samples, log_probs = generate_samples(model, text, n_samples, temperature)\n",
        "\n",
        "    # Compute the overlap scores\n",
        "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
        "\n",
        "    # Find the key with the highest score\n",
        "    max_score_key = max(scores, key = lambda x: scores[x])\n",
        "\n",
        "    translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
        "\n",
        "    best_translation = translations[max_score_key]\n",
        "\n",
        "    return best_translation, translations"
      ],
      "metadata": {
        "id": "cHOa4viexez-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentence = \"do you like playing football\"\n",
        "\n",
        "translation, candidates = mbr_decode(translator, english_sentence, n_samples=10, temperature=0.6)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gifWucdfrNk2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oOUMMDW8oGG-",
        "outputId": "c2c0bc04-1006-447f-cb6b-b6bbdb30065e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'voce gosta de jogar futebol . [EOS]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Translation candidates:\")\n",
        "\n",
        "for c in candidates:\n",
        "    print(c)\n",
        "\n",
        "print(f\"\\nSelected translation: {translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZNUHxfzd1PL",
        "outputId": "a7df1f56-9432-4038-ee98-8d99da5ea427"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation candidates:\n",
            "voce gosta de jogar futebol . [EOS]\n",
            "voce gosta de jogar futebol . [EOS]\n",
            "voces gostam de jogar futebol de futebol . [EOS]\n",
            "voce gosta de jogar futebol . [EOS]\n",
            "voce gosta de jogar futebol . [EOS]\n",
            "voce gosta de jogar futebol . [EOS]\n",
            "voce gosta de jogar futebol . [EOS]\n",
            "voce gosta de jogar futebol . [EOS]\n",
            "voces gostam de jogar futebol . [EOS]\n",
            "voce gosta de jogar futebol ! [EOS]\n",
            "\n",
            "Selected translation: voce gosta de jogar futebol . [EOS]\n"
          ]
        }
      ]
    }
  ]
}