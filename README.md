# NMT-With-Attention
This project implements a Neural Machine Translation (NMT) model with attention. It features an encoder-decoder architecture, custom cross-attention layers, and training with masked loss and accuracy. It includes beam search decoding, token similarity metrics, and Minimum Bayes-Risk decoding for optimal translations.
